{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stevenn9981/tic_tac_toe/blob/master/tic_tac_toe_part1_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEy_PMnnf-Og"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdG6QronfqnR",
        "outputId": "6ebd85f7-cda8-4ebc-ea88-70a75a9b86fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,277 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,472 kB]\n",
            "Fetched 2,867 kB in 2s (1,259 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-6).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install imageio\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-vtSiPUgOOU"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import zipfile\n",
        "import copy\n",
        "\n",
        "import pygame as pg\n",
        "from pygame import gfxdraw\n",
        "from pygame.locals import *\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import categorical_q_network\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import PolicyStep\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "tempdir = \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsV4e7iqg10s"
      },
      "outputs": [],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWwU8Wp7g2_X"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reKJx_ybg7_L"
      },
      "outputs": [],
      "source": [
        "num_iterations = 1200 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "initial_collect_episodes = 5  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "collect_episodes_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 50000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256  # @param {type:\"integer\"}\n",
        "learning_rate = 3e-4  # @param {type:\"number\"}\n",
        "log_interval = 5  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 50  # @param {type:\"integer\"}\n",
        "eval_interval = 20  # @param {type:\"integer\"}\n",
        "\n",
        "gamma = 0    # @param {type:\"number\"}\n",
        "n_step_update = 1  # @param {type:\"integer\"}\n",
        "num_atoms = 51  # @param {type:\"integer\"}\n",
        "min_q_value = -5  # @param {type:\"integer\"}\n",
        "max_q_value = 5  # @param {type:\"integer\"}\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "BOARD_SIZE = 9 # @param {type:\"integer\"}\n",
        "\n",
        "REWARD_DRAW = 0    # @param {type:\"number\"}\n",
        "REWARD_ALIVE = 0    # @param {type:\"number\"}\n",
        "REWARD_NON_ADJ = -0.03    # @param {type:\"number\"}\n",
        "\n",
        "# '_' means empty position, 'O' and 'X' means two players.\n",
        "REWARD_ACTIVE_TWO = 0.1 # @param {type:\"number\"} _OO_ or _XX_, we call it active_two\n",
        "REWARD_NONACT_THREE = 0.25 # @param {type:\"number\"} _OOOX or _XXXO or XOOO_ or OXXX_, we call it non_active_three\n",
        "REWARD_ACTIVE_THREE = 0.6 # @param {type:\"number\"} _OOO_ or _XXX_, we call it active_three\n",
        "REWARD_WIN = 2    # @param {type:\"number\"} _OOOO_ or _XXXX_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOBVDKmMIcyJ"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq7Q2oxyIhS8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "class TicTacToeEnv1(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    Implementation of a TicTacToe Environment based on the instructions of Part 1, Question 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train=False) -> None:\n",
        "        \"\"\"This class contains a TicTacToe environment for OpenAI Gym\n",
        "\n",
        "        Args:\n",
        "            train (bool): whether this is an environment for training.\n",
        "        \"\"\"\n",
        "        self.n_actions = BOARD_SIZE * BOARD_SIZE         # 9 * 9 grids to drop\n",
        "\n",
        "        self._observation_spec = {'state': array_spec.BoundedArraySpec(shape=(BOARD_SIZE, BOARD_SIZE, 4), dtype=np.int_, minimum=0, maximum=1),\n",
        "                                  'legal_moves': array_spec.ArraySpec(shape=(self.n_actions,), dtype=np.bool_)}\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int_, minimum=0, maximum=80)\n",
        "        self.colors = [1, 2]\n",
        "        self.screen = None\n",
        "        self.fields_per_side = BOARD_SIZE\n",
        "        self.train = train\n",
        "        self.reset()\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def get_result(self):\n",
        "        return self.result\n",
        "\n",
        "    def if_chess_nearby(self, row, col) -> bool:\n",
        "        \"\"\"\n",
        "        Determine whether there are chess pieces in 2 squares around the given position (row, col)\n",
        "\n",
        "        Args:\n",
        "          row (int): row index\n",
        "          col (int): column index\n",
        "\n",
        "        Returns:\n",
        "          res (boolean): true, if there are\n",
        "        \"\"\"\n",
        "        for i in range(-2, 3):\n",
        "            for j in range(-2, 3):\n",
        "                if 0 <= row + i < BOARD_SIZE and 0 <= col + j < BOARD_SIZE:\n",
        "                    if self.board[row + i, col + j] != 0:\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def _reset(self) -> Tuple[np.ndarray, dict]:\n",
        "        \"\"\"\n",
        "        reset the board game and state\n",
        "        \"\"\"\n",
        "        self.board: np.ndarray = np.zeros(\n",
        "            (self.fields_per_side, self.fields_per_side), dtype=int\n",
        "        )\n",
        "        self.current_player = 1\n",
        "        self.info = {\"players\": {1: {\"actions\": []}, 2: {\"actions\": []}}, \"Occupied\": set(), \"legal_moves\": np.ones((self.n_actions,), dtype=bool)}\n",
        "        self.latest_action = None\n",
        "\n",
        "        # 0 means not finished, 1 or 2 means the winner and 3 means draw\n",
        "        self.result = 0\n",
        "\n",
        "        # return self.decompose_board_to_state()\n",
        "        observations_and_legal_moves = {'state': self.decompose_board_to_state(), 'legal_moves': self.info[\"legal_moves\"]}\n",
        "        return ts.restart(observations_and_legal_moves)\n",
        "\n",
        "    def decompose_board_to_state(self):\n",
        "        \"\"\"\n",
        "        Our state is a 9x9x4 matrix.\n",
        "        The first layer is the opponent's play history, 0 means no stone, 1 means stones placed by the opponent.\n",
        "        The second layer is the current player's history, 0 means no stone, 1 means stones placed by the current player.\n",
        "        The third layer is the opponent's latest play-out; only one entry is 1 and the others are 0. If the board is empty now, all entries are 0.\n",
        "        The fourth layer is whether the current player is the first hand. an array that is full of 1 means yes, and 0 means no.\n",
        "        \"\"\"\n",
        "        opponent = 2 if self.current_player == 1 else 1\n",
        "        o_plays = (self.board == opponent) * 1\n",
        "        c_plays = (self.board == self.current_player) * 1\n",
        "        l_play = np.zeros_like(self.board)\n",
        "        if self.latest_action:\n",
        "            r, c = self.decode_action(self.latest_action)\n",
        "            l_play[r, c] = 1\n",
        "        if_first = np.full_like(self.board, (self.current_player == 1) * 1)\n",
        "        return np.stack([o_plays, c_plays, l_play, if_first], axis=2)\n",
        "\n",
        "\n",
        "    def _step(self, action: int) -> Tuple[np.ndarray, int, bool, dict]:\n",
        "        \"\"\"step function of the tictactoeEnv1\n",
        "\n",
        "        Args:\n",
        "          action (int): integer between [0, 80], each representing a field on the board\n",
        "\n",
        "        Returns:\n",
        "          state (np.array): state of 2 players' history, 0 means no stone, 1 means stones placed by the corresponding player (shape: 9x9x2).\n",
        "          reward (int): reward of the currrent step\n",
        "          done (boolean): true, if the game is finished\n",
        "          (dict): empty dict for future game related information\n",
        "        \"\"\"\n",
        "        action = int(action)\n",
        "        if not (0 <= action < self.n_actions):\n",
        "            raise ValueError(f\"action '{action}' is not in action_space\")\n",
        "\n",
        "        reward = REWARD_ALIVE\n",
        "        (row, col) = self.decode_action(action)\n",
        "\n",
        "        # If the agent/player does not choose an empty square, raise the ValueError.\n",
        "        if self.board[row, col] != 0:\n",
        "            if len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE:\n",
        "                raise ValueError('BORAD IS FULL!')\n",
        "            raise ValueError('ERROR: Not A LEGAL MOVE (NOT EMPTY)')\n",
        "\n",
        "        # According to the game rules, randomly select an adjacent position with a probability of 1/16.\n",
        "        # Note that since this will bring some randomness to the training process, we disable this during training.\n",
        "        if not self.train:\n",
        "            if random.random() < 0.5:\n",
        "                row, col = self.choose_adj_pos(row, col)\n",
        "\n",
        "        # if len(self.info[\"Occupied\"]) != 0 and not self.if_chess_nearby(row, col):\n",
        "        #     reward += REWARD_NON_ADJ\n",
        "\n",
        "        win = False\n",
        "        if 0 <= row < BOARD_SIZE and 0 <= col < BOARD_SIZE and self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.current_player  # drop the piece on the field\n",
        "            win = self._is_win(self.current_player, row, col)\n",
        "\n",
        "            cnt_act_two = self.detect_alive_two(row, col)\n",
        "            cnt_non_act_three, cnt_act_three = self.detect_three(row, col)\n",
        "\n",
        "            reward += (cnt_act_two * REWARD_ACTIVE_TWO + cnt_non_act_three * REWARD_NONACT_THREE + cnt_act_three * REWARD_ACTIVE_THREE)\n",
        "\n",
        "            action = row * BOARD_SIZE + col\n",
        "            self.latest_action = action\n",
        "            self.info[\"players\"][self.current_player][\"actions\"].append(action)\n",
        "            self.info[\"Occupied\"].add(action)\n",
        "            self.info['legal_moves'][action] = False\n",
        "\n",
        "        if win:\n",
        "            self.result = self.current_player\n",
        "            reward += REWARD_WIN\n",
        "        elif len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE: # Draw\n",
        "            self.result = 3\n",
        "            reward += REWARD_DRAW\n",
        "\n",
        "        done = (win or len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE)\n",
        "        self.current_player = self.current_player + 1 if self.current_player == 1 else 1\n",
        "        state = self.decompose_board_to_state()\n",
        "\n",
        "        observations_and_legal_moves = {'state': state, 'legal_moves': self.info['legal_moves']}\n",
        "\n",
        "        if done:\n",
        "            return ts.termination(observations_and_legal_moves, reward)\n",
        "        else:\n",
        "            return ts.transition(observations_and_legal_moves, reward)\n",
        "\n",
        "    def detect_alive_two(self, row: int, col: int) -> int:\n",
        "        \"\"\" Detect how many alive_two can obtain by this play out.\n",
        "\n",
        "        Args:\n",
        "            row (int): row of the current play\n",
        "            col (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            cnt (int): the number of alive_two\n",
        "        \"\"\"\n",
        "        cnt = 0\n",
        "        adjs = [[-1, -1], [-1, 0], [-1, 1], [0, 1], [0, -1], [1, -1], [1, 0], [1, 1]]\n",
        "        for adj in adjs:\n",
        "            p1_r, p1_c = row + 2 * adj[0], col + 2 * adj[1]\n",
        "            p2_r, p2_c = row - adj[0], col - adj[1]\n",
        "            if 0 <= p1_r < 9 and 0 <= p1_c < 9 and 0 <= p2_r < 9 and 0 <= p2_c < 9:\n",
        "                if self.board[row + adj[0], col + adj[1]] == self.current_player and self.board[p1_r, p1_c] == 0 and self.board[p2_r, p2_c] == 0:\n",
        "                    cnt += 1\n",
        "        return cnt\n",
        "\n",
        "    def detect_three(self, r: int, c: int) -> tuple:\n",
        "        \"\"\" Detect how many non_active_three and active_three can obtain by this play out.\n",
        "\n",
        "        Args:\n",
        "            r (int): row of the current play\n",
        "            c (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            cnt_non_act (int): the number of non_active_three\n",
        "            cnt_act (int): the number of active_three\n",
        "        \"\"\"\n",
        "        cnt_non_act = 0\n",
        "        cnt_act = 0\n",
        "        opponent = self.current_player + 1 if self.current_player == 1 else 1\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-2, 3):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if self.board[r + offset * direct[0], c + offset * direct[1]] == self.current_player:\n",
        "                        count += 1\n",
        "                        if count == 3:\n",
        "                            p1_r, p1_c = r + (offset + 1) * direct[0], c + (offset + 1) * direct[1]\n",
        "                            p2_r, p2_c = r + (offset - 3) * direct[0], c + (offset - 3) * direct[1]\n",
        "\n",
        "                            p1_is_empty = (0 <= p1_r < 9 and 0 <= p1_c < 9 and self.board[p1_r, p1_c] == 0)\n",
        "                            p2_is_empty = (0 <= p2_r < 9 and 0 <= p2_c < 9 and self.board[p2_r, p2_c] == 0)\n",
        "\n",
        "                            if p1_is_empty and p2_is_empty:\n",
        "                                cnt_act += 1\n",
        "                            elif p1_is_empty or p2_is_empty:\n",
        "                                cnt_non_act += 1\n",
        "                            break\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return cnt_non_act, cnt_act\n",
        "\n",
        "    def choose_adj_pos(self, row: int, col: int) -> tuple:\n",
        "        \"\"\" Randomly select an adjacent position with equal probabilities.\n",
        "\n",
        "        Args:\n",
        "            row (int): row of the current play\n",
        "            col (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            row (int): row of the selected adjacent position\n",
        "            col (int): column of the selected adjacent position\n",
        "        \"\"\"\n",
        "\n",
        "        adjs = [[-1, -1], [-1, 0], [-1, 1], [0, 1], [0, -1], [1, -1], [1, 0], [1, 1]]\n",
        "        adj = random.choice(adjs)\n",
        "        row, col = row + adj[0], col + adj[1]\n",
        "        return row, col\n",
        "\n",
        "    def _is_win(self, color: int, r: int, c: int) -> bool:\n",
        "        \"\"\"check if this player results in a winner\n",
        "\n",
        "        Args:\n",
        "            color (int): of the player\n",
        "            r (int): row of the current play\n",
        "            c (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            bool: indicating if there is a winner\n",
        "        \"\"\"\n",
        "\n",
        "        # check if four equal stones are aligned (horizontal, verical or diagonal)\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-3, 4):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if self.board[r + offset * direct[0], c + offset * direct[1]] == color:\n",
        "                        count += 1\n",
        "                        if count == 4:\n",
        "                            return True\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return False\n",
        "\n",
        "    def decode_action(self, action: int) -> List[int]:\n",
        "        \"\"\"decode the action integer into a colum and row value\n",
        "\n",
        "        0 = upper left corner\n",
        "        8 = lower right corner\n",
        "\n",
        "        Args:\n",
        "            action (int): action\n",
        "\n",
        "        Returns:\n",
        "            List[int, int]: a list with the [row, col] values\n",
        "        \"\"\"\n",
        "        col = action % BOARD_SIZE\n",
        "        row = action // BOARD_SIZE\n",
        "        assert 0 <= col < BOARD_SIZE\n",
        "        return [row, col]\n",
        "\n",
        "    def render(self, render_mode=\"rgb_array\") -> None:\n",
        "        \"\"\"Render the board\n",
        "        Print a string that shows the current board, if render_mode == human,\n",
        "        Return the RGB array of a figure which shows the current board.\n",
        "        \"\"\"\n",
        "        board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=str)\n",
        "        for ii in range(BOARD_SIZE):\n",
        "            for jj in range(BOARD_SIZE):\n",
        "                if self.board[ii, jj] == 0:\n",
        "                    board[ii, jj] = \"-\"\n",
        "                elif self.board[ii, jj] == 1:\n",
        "                    board[ii, jj] = \"X\"\n",
        "                elif self.board[ii, jj] == 2:\n",
        "                    board[ii, jj] = \"O\"\n",
        "\n",
        "        if render_mode == \"human\":\n",
        "            board = tabulate(board, tablefmt=\"fancy_grid\")\n",
        "            print(board)\n",
        "            print(\"\\n\")\n",
        "\n",
        "\n",
        "        width = height = 400\n",
        "\n",
        "        white = (255, 255, 255)\n",
        "        line_color = (0, 0, 0)\n",
        "        red = (255, 0, 0)\n",
        "\n",
        "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "        pg.init()\n",
        "\n",
        "        # Set up the drawing window\n",
        "        if self.screen is None:\n",
        "          self.screen = pg.display.set_mode([width + 16, height + 16])\n",
        "\n",
        "        self.screen.fill(white)\n",
        "        # drawing vertical lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (width / BOARD_SIZE * i, 0), (width / BOARD_SIZE * i, height), 2)\n",
        "\n",
        "        # drawing horizontal lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (0, height / BOARD_SIZE * i), (width, height / BOARD_SIZE * i), 2)\n",
        "        pg.display.flip()\n",
        "\n",
        "        lastest_row, latest_col = -1, -1\n",
        "        if self.latest_action:\n",
        "          lastest_row, latest_col = self.decode_action(self.latest_action)\n",
        "\n",
        "        # drawing noughts and crosses\n",
        "        for i in range(BOARD_SIZE):\n",
        "          for j in range(BOARD_SIZE):\n",
        "            color = line_color\n",
        "            if lastest_row == i and latest_col == j:\n",
        "              color = red\n",
        "            if self.board[i, j] == 1: # Draw crosses\n",
        "              pg.draw.lines(self.screen, color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) - 10),\n",
        "                                                      (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10)], 3)\n",
        "              pg.draw.lines(self.screen, color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10),\n",
        "                                                        (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                          height / BOARD_SIZE * (i + 0.5) - 10)], 3)\n",
        "            elif self.board[i, j] == 2: # Draw noughts\n",
        "              pg.draw.circle(self.screen, color, (width / BOARD_SIZE * (j + 0.5), height / BOARD_SIZE * (i + 0.5)), 12, 3)\n",
        "\n",
        "        board = np.transpose(\n",
        "                np.array(pg.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "        return board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhocpw-j36C0"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "    return obs['state'], obs['legal_moves']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFsRLWJslkf"
      },
      "source": [
        "## Random play test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYsgWM-79vr"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"480\" height=\"480\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(eval_env, policy, filename, fps=2):\n",
        "    py_env = eval_env._envs[0]\n",
        "    tf_env = eval_env\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "      time_step = tf_env.reset()\n",
        "      video.append_data(py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step, eval_env)\n",
        "        time_step = tf_env.step(action_step.action)\n",
        "        video.append_data(py_env.render())\n",
        "\n",
        "    return embed_mp4(filename)\n",
        "\n",
        "\n",
        "def create_policy_battle_video(eval_env, policy1, policy2, filename, fps=2):\n",
        "    py_env = eval_env._envs[0]\n",
        "    tf_env = eval_env\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "      time_step = tf_env.reset()\n",
        "      video.append_data(py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy1.action(time_step, eval_env)\n",
        "        time_step = tf_env.step(action_step.action)\n",
        "        video.append_data(py_env.render())\n",
        "        if not time_step.is_last():\n",
        "          action_step = policy2.action(time_step, eval_env)\n",
        "          time_step = tf_env.step(action_step.action)\n",
        "          video.append_data(py_env.render())\n",
        "\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNw07pahBY78"
      },
      "outputs": [],
      "source": [
        "py_env = TicTacToeEnv1()\n",
        "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
        "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)\n",
        "create_policy_eval_video(tf_env, random_policy, \"random-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPHWdRyT-ND0"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnqVY5icuPi4"
      },
      "outputs": [],
      "source": [
        "train_py_env = TicTacToeEnv1(train=True)\n",
        "eval_py_env = TicTacToeEnv1()\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "conv_layer_params = [32, 64, 32]\n",
        "action_tensor_spec = tensor_spec.from_spec(tf_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Conv layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def conv_layer(num_units):\n",
        "  return tf.keras.layers.Conv2D(\n",
        "                      filters=num_units,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "\n",
        "\n",
        "# QNetwork consists of a sequence of Conv layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "normalization1 = tf.keras.layers.BatchNormalization()\n",
        "normalization2 = tf.keras.layers.BatchNormalization()\n",
        "conv_layers = [conv_layer(num_units) for num_units in conv_layer_params]\n",
        "action_conv = tf.keras.layers.Conv2D(filters=4,\n",
        "                    kernel_size=[1, 1], padding=\"same\",\n",
        "                    data_format=\"channels_last\")\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential([normalization1] + conv_layers + [action_conv, normalization2, flatten, q_values_layer])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PlayPolicy():\n",
        "    def __init__(self, policy):\n",
        "        self.policy = policy\n",
        "    def action(self, act: int, env=None):\n",
        "        action_step = self.policy.action(act)\n",
        "        if env:\n",
        "            # when the board is empty, choose the center position.\n",
        "            py_env = env._envs[0]\n",
        "            if np.sum(py_env.board) == 0:\n",
        "                return PolicyStep(action=tf.convert_to_tensor(np.array([40])))\n",
        "            elif np.sum(py_env.board) > 6:\n",
        "                current_player = py_env.current_player\n",
        "                opponent = current_player + 1 if current_player == 1 else 1\n",
        "                occupied_positions = py_env.info['Occupied']\n",
        "                for act in range(81):\n",
        "                    if act not in occupied_positions and self.drop_here_will_win(py_env, act, current_player):\n",
        "                        return PolicyStep(action=tf.convert_to_tensor(np.array([act])))\n",
        "                for act in range(81):\n",
        "                    if act not in occupied_positions and self.drop_here_will_win(py_env, act, opponent):\n",
        "                        return PolicyStep(action=tf.convert_to_tensor(np.array([act])))\n",
        "        return action_step\n",
        "\n",
        "    def drop_here_will_win(self, py_env, action, color):\n",
        "        # check if four equal stones are aligned (horizontal, verical or diagonal)\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        current_board = copy.deepcopy(py_env.board)\n",
        "        r, c = py_env.decode_action(action)\n",
        "        current_board[r, c] = color\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-3, 4):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if current_board[r + offset * direct[0], c + offset * direct[1]] == color:\n",
        "                        count += 1\n",
        "                        if count == 4:\n",
        "                            return True\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "GfDHHq84JCYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg5emNEam75J"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "agent1 = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    gamma=gamma,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=tf.Variable(0))\n",
        "\n",
        "agent2 = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    gamma=gamma,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=tf.Variable(0))\n",
        "\n",
        "agent1.initialize()\n",
        "agent2.initialize()\n",
        "\n",
        "play_policy1 = PlayPolicy(agent1.policy)\n",
        "play_policy2 = PlayPolicy(agent2.policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVnWWX4xpa5"
      },
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGU0pGS4xsjc"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8vRFWNgx-tj"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "def compute_avg_return_battle(environment, policy1, policy2, num_episodes=10):\n",
        "\n",
        "  total_return_1 = 0.0\n",
        "  total_return_2 = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    episode_return_1 = 0.0\n",
        "    episode_return_2 = 0.0\n",
        "    time_step = environment.reset()\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy1.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return_1 += time_step.reward\n",
        "      if not time_step.is_last():\n",
        "        action_step = policy2.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "        episode_return_2 += time_step.reward\n",
        "    total_return_1 += episode_return_1\n",
        "    total_return_2 += episode_return_2\n",
        "\n",
        "  avg_return_1 = total_return_1 / num_episodes\n",
        "  avg_return_2 = total_return_2 / num_episodes\n",
        "  return [avg_return_1.numpy()[0], avg_return_2.numpy()[0]]\n",
        "\n",
        "def compute_avg_win_battle(environment, policy1, policy2, num_episodes=10):\n",
        "\n",
        "  total_return_1 = 0.0\n",
        "  total_return_2 = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy1.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      if not time_step.is_last():\n",
        "        action_step = policy2.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "    if environment._envs[0].get_result() == 1:\n",
        "        total_return_1 += 1\n",
        "    elif environment._envs[0].get_result() == 2:\n",
        "        total_return_2 += 1\n",
        "\n",
        "  avg_return_1 = total_return_1 / num_episodes\n",
        "  avg_return_2 = total_return_2 / num_episodes\n",
        "  return [avg_return_1, avg_return_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCXIdRVyAAu"
      },
      "outputs": [],
      "source": [
        "print(compute_avg_return(eval_env, play_policy1, 1))\n",
        "print(compute_avg_return(eval_env, play_policy2, 1))\n",
        "\n",
        "print(compute_avg_return_battle(eval_env, play_policy1, play_policy2, 10))\n",
        "print(compute_avg_win_battle(eval_env, play_policy1, play_policy2, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBEXcoMjy7xO"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMVmKJF1y_Lt"
      },
      "outputs": [],
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                                          data_spec=agent1.collect_data_spec,\n",
        "                                          batch_size=train_env.batch_size,\n",
        "                                          max_length=replay_buffer_max_length)\n",
        "\n",
        "def collect_episode(environment, agent1, agent2, replay_buffer):\n",
        "    time_step = environment.reset()\n",
        "    trajs_1, trajs_2 = [], []\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = agent1.collect_policy.action(time_step)\n",
        "      next_time_step = environment.step(action_step.action)\n",
        "      traj1 = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "      trajs_1.append(traj1)\n",
        "      time_step = next_time_step\n",
        "\n",
        "      if not time_step.is_last():\n",
        "        action_step = agent2.collect_policy.action(time_step)\n",
        "        next_time_step = environment.step(action_step.action)\n",
        "        traj2 = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "        trajs_2.append(traj2)\n",
        "        time_step = next_time_step\n",
        "\n",
        "    change_flag = False\n",
        "    # Modify the reward of each step according to the opponent's next step\n",
        "    if len(trajs_1) == len(trajs_2): # Player 2 won\n",
        "        for i in range(len(trajs_1) - 1):\n",
        "            trajs_1[i] = trajs_1[i].replace(reward=trajs_1[i].reward - trajs_2[i].reward)\n",
        "            trajs_2[i] = trajs_2[i].replace(reward=trajs_2[i].reward - trajs_1[i + 1].reward)\n",
        "        trajs_1[-1] = trajs_1[-1].replace(reward=trajs_1[-1].reward - trajs_2[-1].reward)\n",
        "    else: # Player 1 won\n",
        "        change_flag = True\n",
        "        for i in range(len(trajs_1) - 1):\n",
        "            trajs_1[i] = trajs_1[i].replace(reward=trajs_1[i].reward - trajs_2[i].reward)\n",
        "            trajs_2[i] = trajs_2[i].replace(reward=trajs_2[i].reward - trajs_1[i + 1].reward)\n",
        "\n",
        "    for i in range(len(trajs_1)):\n",
        "        replay_buffer.add_batch(trajs_1[i])\n",
        "    for i in range(len(trajs_2)):\n",
        "        replay_buffer.add_batch(trajs_2[i])\n",
        "\n",
        "    # If the first player wins, then change the order in the next game.\n",
        "    return change_flag\n",
        "\n",
        "for _ in range(initial_collect_episodes):\n",
        "    collect_episode(train_env, agent1, agent2, replay_buffer)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
        "    num_steps=n_step_update + 1).prefetch(3)\n",
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmMc8b6mBqli"
      },
      "source": [
        "## Set up the checkpointer and Policy saver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRLhR6v4BsEb"
      },
      "outputs": [],
      "source": [
        "def create_zip_file(dirname, base_filename):\n",
        "  return shutil.make_archive(base_filename, 'zip', dirname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI2LVHZXBv4q"
      },
      "outputs": [],
      "source": [
        "policy_dir = os.path.join(tempdir, 'play_policy')\n",
        "tf_policy_saver = policy_saver.PolicySaver(play_policy1.policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRVMc8V8zxj0"
      },
      "source": [
        "## Train the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1qWkmKz0Ke"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent1.train = common.function(agent1.train)\n",
        "agent2.train = common.function(agent2.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent1.train_step_counter.assign(0)\n",
        "agent2.train_step_counter.assign(0)\n",
        "policy_win_rate = compute_avg_win_battle(eval_env, play_policy1, random_policy, num_eval_episodes)[0]\n",
        "print('Before training: 1_win = {0}'.format(policy_win_rate))\n",
        "\n",
        "bst = 0\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_env.reset()\n",
        "\n",
        "x, y, change_flag = agent1, agent2, False\n",
        "for idx in range(num_iterations):\n",
        "    # Collect a few episodes using collect_policy and store the transitions to the replay buffer.\n",
        "    for _ in range(collect_episodes_per_iteration):\n",
        "      if change_flag:\n",
        "        x, y = y, x\n",
        "      change_flag = collect_episode(train_env, x, y, replay_buffer)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss1 = agent1.train(experience).loss\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss2 = agent2.train(experience).loss\n",
        "\n",
        "    step = agent1.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "      print('step = {0}: loss1 = {1}'.format(step, train_loss1))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      policy_win_rate1 = compute_avg_win_battle(eval_env, play_policy1, random_policy, num_eval_episodes)[0]\n",
        "      policy_win_rate2 = compute_avg_win_battle(eval_env, random_policy, play_policy1, num_eval_episodes)[1]\n",
        "      battle_win_rate = compute_avg_win_battle(eval_env, play_policy2, play_policy1, 10)\n",
        "      print('Evaluation (step = {0}): offense_win = {1}, defense_win = {2}, battle_result = {3}'.format(step, policy_win_rate1, policy_win_rate2, battle_win_rate))\n",
        "      policy_win_rate = (policy_win_rate1 + policy_win_rate2) / 2\n",
        "      if policy_win_rate >= bst:\n",
        "        bst = policy_win_rate\n",
        "        tf_policy_saver.save(policy_dir)\n",
        "        policy_zip_filename = create_zip_file(policy_dir, os.path.join(tempdir, 'exported_policy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOj19B0B1ky"
      },
      "source": [
        "## Load the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSRWvH6Udnd1"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDMa0xH3rAS6"
      },
      "outputs": [],
      "source": [
        "play_policy = PlayPolicy(tf.saved_model.load(policy_dir))\n",
        "create_policy_battle_video(eval_env, play_policy, random_policy, 'trained-agent-1-with-random')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPyiZXnNlQVW"
      },
      "outputs": [],
      "source": [
        "create_policy_battle_video(eval_env, play_policy, play_policy, 'trained-agent-1-battle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsl0r1fyxpUD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkOaR2zHrDwC"
      },
      "source": [
        "## TODO\n",
        "\n",
        "\n",
        "\n",
        "*   Check whether all functions are correct\n",
        "*   Check whether the environment is correctly implement\n",
        "*   Add the variant in `step` function\n",
        "*   Test whether the agent always chooses an *empty* square and whether they can go to an adjacent place correctly\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXHgPU5Bv3WJ6N9iIcAGXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}