{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs6Gg8jIskpzRsQbdx9Vr9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stevenn9981/tic_tac_toe/blob/master/tic_tac_toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "iEy_PMnnf-Og"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdG6QronfqnR",
        "outputId": "098e6287-a65f-4982-f6af-aed1070ead59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (52.85.151.129)] [Co\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.5\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-6).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n",
            "Requirement already satisfied: imageio==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "import random\n",
        "import os\n",
        "\n",
        "import pygame as pg\n",
        "from pygame import gfxdraw\n",
        "from pygame.locals import *\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "metadata": {
        "id": "h-vtSiPUgOOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "id": "MsV4e7iqg10s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters"
      ],
      "metadata": {
        "id": "sWwU8Wp7g2_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 500000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "BOARD_SIZE = 9 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "reKJx_ybg7_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "WOBVDKmMIcyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Implementation of a TicTacToe Environment based on OpenAI Gym standards\n",
        "    This class is modified from https://github.com/MauroLuzzatto/OpenAI-Gym-TicTacToe-Environment\n",
        "    I modified that to follow the instructions of Question 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='rgb_array') -> None:\n",
        "        \"\"\"This class contains a TicTacToe environment for OpenAI Gym\n",
        "\n",
        "        Args:\n",
        "            small (int): small reward for each move (punishment)\n",
        "            large (int): large reward for game winner\n",
        "        \"\"\"\n",
        "        n_actions = BOARD_SIZE * BOARD_SIZE         # 9 * 9 grids to drop\n",
        "        self.action_space = gym.spaces.Discrete(n_actions)\n",
        "        # state: 9 * 9 * 2: the first 9 * 9 layer is the opponent's play history and the second 9 * 9 layer is ours.\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(BOARD_SIZE, BOARD_SIZE, 2), dtype=float)\n",
        "        self.render_mode = mode\n",
        "        self.colors = [1, 2]\n",
        "        self.screen = None\n",
        "        self.fields_per_side = int(math.sqrt(n_actions))\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> Tuple[np.ndarray, dict]:\n",
        "        \"\"\"\n",
        "        reset the board game and state\n",
        "        \"\"\"\n",
        "        self.board: np.ndarray = np.zeros(\n",
        "            (self.fields_per_side, self.fields_per_side), dtype=int\n",
        "        )\n",
        "        self.current_player = 1\n",
        "        self.info = {\"players\": {1: {\"actions\": []}, 2: {\"actions\": []}}, \"Occupied\": set()}\n",
        "\n",
        "        if self.render_mode == 'human':\n",
        "            self.render(self.render_mode)\n",
        "        return self.decompose_board_to_state()\n",
        "\n",
        "    def decompose_board_to_state(self):\n",
        "        \"\"\"\n",
        "        Our state is a 9x9x2 matrix.\n",
        "        The first layer is the opponent's play history, 0 means no stone, 1 means stones placed by the opponent.\n",
        "        The second layer is the current player's history, 0 means no stone, 1 means stones placed by the current player.\n",
        "        \"\"\"\n",
        "        opponent = 2 if self.current_player == 1 else 1\n",
        "        o_plays = (self.board == opponent) * 1\n",
        "        c_plays = (self.board == self.current_player) * 1\n",
        "        return np.stack([o_plays, c_plays], axis=2)\n",
        "\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, int, bool, dict]:\n",
        "        \"\"\"step function of the tictactoeEnv\n",
        "\n",
        "        Args:\n",
        "          action (int): integer between 0-80, each representing a field on the board\n",
        "\n",
        "        Returns:\n",
        "          state (np.array): state of 2 players' history, 0 means no stone, 1 means stones placed by the corresponding player (shape: 9x9x2).\n",
        "          reward (int): reward of the currrent step\n",
        "          done (boolean): true, if the game is finished\n",
        "          (dict): empty dict for future game related information\n",
        "        \"\"\"\n",
        "        if not self.action_space.contains(action):\n",
        "            raise ValueError(f\"action '{action}' is not in action_space\")\n",
        "\n",
        "        reward = -0.1  # assign (negative) reward for every move done\n",
        "        (row, col) = self.decode_action(action)\n",
        "\n",
        "        # If the agent/player does not choose an empty square, randomly select an empty one.\n",
        "        if self.board[row, col] != 0:\n",
        "            action = random.choice(list(set(range(BOARD_SIZE * BOARD_SIZE)) - self.info[\"Occupied\"]))\n",
        "            (row, col) = self.decode_action(action)\n",
        "            reward -= 0.3 # assign a negative reward if the play out position is not empty\n",
        "\n",
        "\n",
        "        # randomly select an adjacent position with probability 1/16\n",
        "        if random.random() < 0.5:\n",
        "            adjs = [[-1, -1], [-1, 0], [-1, 1], [0, 1], [0, -1], [1, -1], [1, 0], [1, 1]]\n",
        "            adj = random.choice(adjs)\n",
        "            row, col = row + adj[0], col + adj[1]\n",
        "\n",
        "        if 0 <= row < BOARD_SIZE and 0 <= col < BOARD_SIZE and self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.current_player  # postion the token on the field\n",
        "            win = self._is_win(self.current_player, row, col)\n",
        "\n",
        "            action = row * BOARD_SIZE + col\n",
        "            self.info[\"players\"][self.current_player][\"actions\"].append(action)\n",
        "            self.info[\"Occupied\"].add(action)\n",
        "        else:\n",
        "            win = False\n",
        "\n",
        "        if win:\n",
        "            reward += 3\n",
        "\n",
        "        done = (win or len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE)\n",
        "        self.current_player = self.current_player + 1 if self.current_player == 1 else 1\n",
        "        state = self.decompose_board_to_state()\n",
        "        return state, reward, done, self.info\n",
        "\n",
        "    def _is_win(self, color: int, r: int, c: int) -> bool:\n",
        "        \"\"\"check if this player results in a winner\n",
        "\n",
        "        Args:\n",
        "            color (int): of the player\n",
        "            r (int): row of the current play\n",
        "            c (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            bool: indicating if there is a winner\n",
        "        \"\"\"\n",
        "\n",
        "        # check if four equal stones are aligned (horizontal, verical or diagonal)\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-3, 4):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if self.board[r + offset * direct[0], c + offset * direct[1]] == color:\n",
        "                        count += 1\n",
        "                        if count == 4:\n",
        "                            return True\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return False\n",
        "\n",
        "    def decode_action(self, action: int) -> List[int]:\n",
        "        \"\"\"decode the action integer into a colum and row value\n",
        "\n",
        "        0 = upper left corner\n",
        "        8 = lower right corner\n",
        "\n",
        "        Args:\n",
        "            action (int): action\n",
        "\n",
        "        Returns:\n",
        "            List[int, int]: a list with the [row, col] values\n",
        "        \"\"\"\n",
        "        col = action % BOARD_SIZE\n",
        "        row = action // BOARD_SIZE\n",
        "        assert 0 <= col < BOARD_SIZE\n",
        "        return [row, col]\n",
        "\n",
        "    def render(self, mode=\"human\") -> None:\n",
        "        \"\"\"render the board\n",
        "\n",
        "        The following charachters are used to represent the fields,\n",
        "            '-' no stone\n",
        "            'O' for player 0\n",
        "            'X' for player 1\n",
        "\n",
        "        An example for a 3x3 game board:\n",
        "            ╒═══╤═══╤═══╕\n",
        "            │ O │ - │ - │\n",
        "            ├───┼───┼───┤\n",
        "            │ - │ X │ - │\n",
        "            ├───┼───┼───┤\n",
        "            │ - │ - │ - │\n",
        "            ╘═══╧═══╧═══╛\n",
        "        \"\"\"\n",
        "        board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=str)\n",
        "        for ii in range(BOARD_SIZE):\n",
        "            for jj in range(BOARD_SIZE):\n",
        "                if self.board[ii, jj] == 0:\n",
        "                    board[ii, jj] = \"-\"\n",
        "                elif self.board[ii, jj] == 1:\n",
        "                    board[ii, jj] = \"X\"\n",
        "                elif self.board[ii, jj] == 2:\n",
        "                    board[ii, jj] = \"O\"\n",
        "\n",
        "        if mode == \"human\":\n",
        "            board = tabulate(board, tablefmt=\"fancy_grid\")\n",
        "            print(board)\n",
        "            print(\"\\n\")\n",
        "\n",
        "\n",
        "        width = height = 400\n",
        "\n",
        "        white = (255, 255, 255)\n",
        "        line_color = (0, 0, 0)\n",
        "\n",
        "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "        pg.init()\n",
        "\n",
        "        # Set up the drawing window\n",
        "        if self.screen is None:\n",
        "          self.screen = pg.display.set_mode([width + 16, height + 16])\n",
        "\n",
        "        self.screen.fill(white)\n",
        "        # drawing vertical lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (width / BOARD_SIZE * i, 0), (width / BOARD_SIZE * i, height), 2)\n",
        "\n",
        "        # drawing horizontal lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (0, height / BOARD_SIZE * i), (width, height / BOARD_SIZE * i), 2)\n",
        "        pg.display.flip()\n",
        "\n",
        "        # drawing noughts and crosses\n",
        "        for i in range(BOARD_SIZE):\n",
        "          for j in range(BOARD_SIZE):\n",
        "            if self.board[i, j] == 1: # Draw crosses\n",
        "              pg.draw.lines(self.screen, line_color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) - 10),\n",
        "                                                      (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10)], 3)\n",
        "              pg.draw.lines(self.screen, line_color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10),\n",
        "                                                        (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                          height / BOARD_SIZE * (i + 0.5) - 10)], 3)\n",
        "            elif self.board[i, j] == 2: # Draw noughts\n",
        "              pg.draw.circle(self.screen, line_color, (width / BOARD_SIZE * (j + 0.5), height / BOARD_SIZE * (i + 0.5)), 12, 3)\n",
        "\n",
        "        board = np.transpose(\n",
        "                np.array(pg.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "        return board"
      ],
      "metadata": {
        "id": "aq7Q2oxyIhS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random play test"
      ],
      "metadata": {
        "id": "LqFsRLWJslkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"480\" height=\"480\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, fps=2):\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "      time_step = tf_env.reset()\n",
        "      video.append_data(py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = tf_env.step(action_step.action)\n",
        "        video.append_data(py_env.render())\n",
        "\n",
        "    return embed_mp4(filename)"
      ],
      "metadata": {
        "id": "qJYsgWM-79vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "py_env = suite_gym.wrap_env(TicTacToeEnv())\n",
        "tf_env = tf_py_environment.TFPyEnvironment(py_env)"
      ],
      "metadata": {
        "id": "uU1h9wSWa7Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\n",
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "metadata": {
        "id": "yNw07pahBY78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "GPHWdRyT-ND0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_py_env = suite_gym.wrap_env(TicTacToeEnv())\n",
        "eval_py_env = suite_gym.wrap_env(TicTacToeEnv())\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "metadata": {
        "id": "MnqVY5icuPi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "conv_layer_params = (32, 64, 32)\n",
        "action_tensor_spec = tensor_spec.from_spec(tf_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Conv layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def conv_layer(num_units):\n",
        "  return tf.keras.layers.Conv2D(filters=num_units,\n",
        "                                kernel_size=[3, 3],\n",
        "                                padding=\"same\",\n",
        "                                data_format=\"channels_last\",\n",
        "                                activation='relu')\n",
        "\n",
        "\n",
        "# QNetwork consists of a sequence of Conv layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "conv_layers = [conv_layer(num_units) for num_units in conv_layer_params]\n",
        "action_conv = tf.keras.layers.Conv2D(filters=4,\n",
        "                                      kernel_size=[1, 1], padding=\"same\",\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      activation='relu')\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(conv_layers + [action_conv, flatten, q_values_layer])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "id": "lg5emNEam75J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy"
      ],
      "metadata": {
        "id": "QaVnWWX4xpa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
      ],
      "metadata": {
        "id": "uGU0pGS4xsjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for i in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "metadata": {
        "id": "N8vRFWNgx-tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_avg_return(eval_env, agent.policy, num_eval_episodes)"
      ],
      "metadata": {
        "id": "vlCXIdRVyAAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Buffer"
      ],
      "metadata": {
        "id": "VBEXcoMjy7xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "metadata": {
        "id": "CMVmKJF1y_Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "py_driver.PyDriver(\n",
        "    py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ],
      "metadata": {
        "id": "RomK5WUjtYbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the agent"
      ],
      "metadata": {
        "id": "jRVMc8V8zxj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "    # Collect a few steps and save to the replay buffer.\n",
        "    time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "      print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "      print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "      returns.append(avg_return)"
      ],
      "metadata": {
        "id": "6_1qWkmKz0Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "metadata": {
        "id": "HDMa0xH3rAS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fsl0r1fyxpUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO\n",
        "\n",
        "\n",
        "\n",
        "*   Check whether the `is_winner` function is correct\n",
        "*   Check whether the environment is correctly implement\n",
        "*   Add the variant in `step` function\n",
        "*   Test whether the agent always chooses an *empty* square and whether they can go to an adjacent place correctly\n",
        "\n"
      ],
      "metadata": {
        "id": "WkOaR2zHrDwC"
      }
    }
  ]
}