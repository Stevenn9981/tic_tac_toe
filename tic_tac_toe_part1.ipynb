{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stevenn9981/tic_tac_toe/blob/master/tic_tac_toe_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEy_PMnnf-Og"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdG6QronfqnR",
        "outputId": "11c0a065-911c-4b3d-f21f-5585b512c838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r            \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Waiting for headers] [2 InRelease 22.9 kB/110 kB 21%] [Connected to cloud.r\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r0% [3 InRelease 56.2 kB/119 kB 47%] [2 InRelease 22.9 kB/110 kB 21%] [Connected\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [3 InRelease 118 kB/119 kB 100%] [2 InRelease 66.3 kB/110 kB 60%] [Waiting f\r0% [Waiting for headers] [2 InRelease 66.3 kB/110 kB 60%] [Waiting for headers]\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,279 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,455 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,186 kB]\n",
            "Fetched 5,271 kB in 4s (1,503 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-6).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.22.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.13.0)\n",
            "Requirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-agents[reverb]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.13.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.2.2)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.10/dist-packages (2.0.9)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install imageio\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h-vtSiPUgOOU"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import zipfile\n",
        "\n",
        "import pygame as pg\n",
        "from pygame import gfxdraw\n",
        "from pygame.locals import *\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import categorical_q_network\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "tempdir = \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MsV4e7iqg10s",
        "outputId": "d1000f73-5fb9-46fd-8429-0d20c17b7fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.14.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWwU8Wp7g2_X"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "reKJx_ybg7_L"
      },
      "outputs": [],
      "source": [
        "num_iterations = 50000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 30 # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256  # @param {type:\"integer\"}\n",
        "learning_rate = 3e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "gamma = 0.99\n",
        "n_step_update = 2  # @param {type:\"integer\"}\n",
        "num_atoms = 51  # @param {type:\"integer\"}\n",
        "min_q_value = -20  # @param {type:\"integer\"}\n",
        "max_q_value = 20  # @param {type:\"integer\"}\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "BOARD_SIZE = 9 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOBVDKmMIcyJ"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aq7Q2oxyIhS8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "class TicTacToeEnv1(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    Implementation of a TicTacToe Environment based on OpenAI Gym standards\n",
        "    This class is modified from https://github.com/MauroLuzzatto/OpenAI-Gym-TicTacToe-Environment\n",
        "    I modified that to follow the instructions of Question 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='rgb_array') -> None:\n",
        "        \"\"\"This class contains a TicTacToe environment for OpenAI Gym\n",
        "\n",
        "        Args:\n",
        "            mode (str): render mode: human or rgb_array, which returns a text or RGB array of a picture that shows the current board\n",
        "        \"\"\"\n",
        "        self.n_actions = BOARD_SIZE * BOARD_SIZE         # 9 * 9 grids to drop\n",
        "        # state: 9 * 9 * 2: the first 9 * 9 layer is the opponent's play history and the second 9 * 9 layer is own.\n",
        "        self._observation_spec = {'state': array_spec.BoundedArraySpec(shape=(BOARD_SIZE, BOARD_SIZE, 2), dtype=np.int_, minimum=0, maximum=1),\n",
        "                                  'legal_moves': array_spec.ArraySpec(shape=(self.n_actions,), dtype=np.bool_)}\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int_, minimum=0, maximum=80)\n",
        "        self.render_mode = mode\n",
        "        self.colors = [1, 2]\n",
        "        self.screen = None\n",
        "        self.gamma = gamma\n",
        "        self.fields_per_side = BOARD_SIZE\n",
        "        self.reset()\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def _reset(self) -> Tuple[np.ndarray, dict]:\n",
        "        \"\"\"\n",
        "        reset the board game and state\n",
        "        \"\"\"\n",
        "        self.board: np.ndarray = np.zeros(\n",
        "            (self.fields_per_side, self.fields_per_side), dtype=int\n",
        "        )\n",
        "        self.current_player = 1\n",
        "        self.info = {\"players\": {1: {\"actions\": []}, 2: {\"actions\": []}}, \"Occupied\": set(), \"legal_moves\": np.ones((self.n_actions,), dtype=bool)}\n",
        "\n",
        "        # return self.decompose_board_to_state()\n",
        "        observations_and_legal_moves = {'state': self.decompose_board_to_state(), 'legal_moves': self.info[\"legal_moves\"]}\n",
        "        return ts.restart(observations_and_legal_moves)\n",
        "\n",
        "    def decompose_board_to_state(self):\n",
        "        \"\"\"\n",
        "        Our state is a 9x9x2 matrix.\n",
        "        The first layer is the opponent's play history, 0 means no stone, 1 means stones placed by the opponent.\n",
        "        The second layer is the current player's history, 0 means no stone, 1 means stones placed by the current player.\n",
        "        \"\"\"\n",
        "        opponent = 2 if self.current_player == 1 else 1\n",
        "        o_plays = (self.board == opponent) * 1\n",
        "        c_plays = (self.board == self.current_player) * 1\n",
        "        return np.stack([o_plays, c_plays], axis=2)\n",
        "\n",
        "\n",
        "    def _step(self, action: int) -> Tuple[np.ndarray, int, bool, dict]:\n",
        "        \"\"\"step function of the tictactoeEnv1\n",
        "\n",
        "        Args:\n",
        "          action (int): integer between [0, 80], each representing a field on the board\n",
        "\n",
        "        Returns:\n",
        "          state (np.array): state of 2 players' history, 0 means no stone, 1 means stones placed by the corresponding player (shape: 9x9x2).\n",
        "          reward (int): reward of the currrent step\n",
        "          done (boolean): true, if the game is finished\n",
        "          (dict): empty dict for future game related information\n",
        "        \"\"\"\n",
        "        action = int(action)\n",
        "        if not (0 <= action < self.n_actions):\n",
        "            raise ValueError(f\"action '{action}' is not in action_space\")\n",
        "\n",
        "        reward = -0.1  # assign (negative) reward for every move done\n",
        "        (row, col) = self.decode_action(action)\n",
        "\n",
        "        # If the agent/player does not choose an empty square, randomly select an empty one.\n",
        "        if self.board[row, col] != 0:\n",
        "            print('WARNING: Not a legal step')\n",
        "            action = random.choice(list(set(range(BOARD_SIZE * BOARD_SIZE)) - self.info[\"Occupied\"]))\n",
        "            (row, col) = self.decode_action(action)\n",
        "            reward -= 1 # assign a negative reward if the play-out position is not empty\n",
        "\n",
        "\n",
        "        # randomly select an adjacent position with probability 1/16\n",
        "        if random.random() < 0.5:\n",
        "            adjs = [[-1, -1], [-1, 0], [-1, 1], [0, 1], [0, -1], [1, -1], [1, 0], [1, 1]]\n",
        "            adj = random.choice(adjs)\n",
        "            row, col = row + adj[0], col + adj[1]\n",
        "\n",
        "        if 0 <= row < BOARD_SIZE and 0 <= col < BOARD_SIZE and self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.current_player  # drop the piece on the field\n",
        "            win = self._is_win(self.current_player, row, col)\n",
        "\n",
        "            action = row * BOARD_SIZE + col\n",
        "            self.info[\"players\"][self.current_player][\"actions\"].append(action)\n",
        "            self.info[\"Occupied\"].add(action)\n",
        "            self.info['legal_moves'][action] = False\n",
        "        else:\n",
        "            win = False\n",
        "\n",
        "        if win:\n",
        "            reward += 3\n",
        "\n",
        "        done = (win or len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE)\n",
        "        self.current_player = self.current_player + 1 if self.current_player == 1 else 1\n",
        "        state = self.decompose_board_to_state()\n",
        "\n",
        "        observations_and_legal_moves = {'state': state, 'legal_moves': self.info['legal_moves']}\n",
        "        # return next_s, reward, done, self.info\n",
        "        if done:\n",
        "            return ts.termination(observations_and_legal_moves, reward)\n",
        "        else:\n",
        "            return ts.transition(observations_and_legal_moves, reward, self.gamma)\n",
        "\n",
        "    def _is_win(self, color: int, r: int, c: int) -> bool:\n",
        "        \"\"\"check if this player results in a winner\n",
        "\n",
        "        Args:\n",
        "            color (int): of the player\n",
        "            r (int): row of the current play\n",
        "            c (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            bool: indicating if there is a winner\n",
        "        \"\"\"\n",
        "\n",
        "        # check if four equal stones are aligned (horizontal, verical or diagonal)\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-3, 4):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if self.board[r + offset * direct[0], c + offset * direct[1]] == color:\n",
        "                        count += 1\n",
        "                        if count == 4:\n",
        "                            return True\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return False\n",
        "\n",
        "    def decode_action(self, action: int) -> List[int]:\n",
        "        \"\"\"decode the action integer into a colum and row value\n",
        "\n",
        "        0 = upper left corner\n",
        "        8 = lower right corner\n",
        "\n",
        "        Args:\n",
        "            action (int): action\n",
        "\n",
        "        Returns:\n",
        "            List[int, int]: a list with the [row, col] values\n",
        "        \"\"\"\n",
        "        col = action % BOARD_SIZE\n",
        "        row = action // BOARD_SIZE\n",
        "        assert 0 <= col < BOARD_SIZE\n",
        "        return [row, col]\n",
        "\n",
        "    def render(self) -> None:\n",
        "        \"\"\"render the board\n",
        "\n",
        "        The following charachters are used to represent the fields,\n",
        "            '-' no stone\n",
        "            'O' for player 0\n",
        "            'X' for player 1\n",
        "\n",
        "        An example for a 3x3 game board:\n",
        "            ╒═══╤═══╤═══╕\n",
        "            │ O │ - │ - │\n",
        "            ├───┼───┼───┤\n",
        "            │ - │ X │ - │\n",
        "            ├───┼───┼───┤\n",
        "            │ - │ - │ - │\n",
        "            ╘═══╧═══╧═══╛\n",
        "        \"\"\"\n",
        "        board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=str)\n",
        "        for ii in range(BOARD_SIZE):\n",
        "            for jj in range(BOARD_SIZE):\n",
        "                if self.board[ii, jj] == 0:\n",
        "                    board[ii, jj] = \"-\"\n",
        "                elif self.board[ii, jj] == 1:\n",
        "                    board[ii, jj] = \"X\"\n",
        "                elif self.board[ii, jj] == 2:\n",
        "                    board[ii, jj] = \"O\"\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            board = tabulate(board, tablefmt=\"fancy_grid\")\n",
        "            print(board)\n",
        "            print(\"\\n\")\n",
        "\n",
        "\n",
        "        width = height = 400\n",
        "\n",
        "        white = (255, 255, 255)\n",
        "        line_color = (0, 0, 0)\n",
        "\n",
        "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "        pg.init()\n",
        "\n",
        "        # Set up the drawing window\n",
        "        if self.screen is None:\n",
        "          self.screen = pg.display.set_mode([width + 16, height + 16])\n",
        "\n",
        "        self.screen.fill(white)\n",
        "        # drawing vertical lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (width / BOARD_SIZE * i, 0), (width / BOARD_SIZE * i, height), 2)\n",
        "\n",
        "        # drawing horizontal lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (0, height / BOARD_SIZE * i), (width, height / BOARD_SIZE * i), 2)\n",
        "        pg.display.flip()\n",
        "\n",
        "        # drawing noughts and crosses\n",
        "        for i in range(BOARD_SIZE):\n",
        "          for j in range(BOARD_SIZE):\n",
        "            if self.board[i, j] == 1: # Draw crosses\n",
        "              pg.draw.lines(self.screen, line_color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) - 10),\n",
        "                                                      (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10)], 3)\n",
        "              pg.draw.lines(self.screen, line_color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10),\n",
        "                                                        (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                          height / BOARD_SIZE * (i + 0.5) - 10)], 3)\n",
        "            elif self.board[i, j] == 2: # Draw noughts\n",
        "              pg.draw.circle(self.screen, line_color, (width / BOARD_SIZE * (j + 0.5), height / BOARD_SIZE * (i + 0.5)), 12, 3)\n",
        "\n",
        "        board = np.transpose(\n",
        "                np.array(pg.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "        return board"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "    return obs['state'], obs['legal_moves']"
      ],
      "metadata": {
        "id": "Yhocpw-j36C0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFsRLWJslkf"
      },
      "source": [
        "## Random play test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uU1h9wSWa7Lh"
      },
      "outputs": [],
      "source": [
        "# py_env = suite_gym.wrap_env(TicTacToeEnv1())\n",
        "py_env = TicTacToeEnv1()\n",
        "tf_env = tf_py_environment.TFPyEnvironment(py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJYsgWM-79vr"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"480\" height=\"480\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, fps=2):\n",
        "    py_env = TicTacToeEnv1()\n",
        "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "      time_step = tf_env.reset()\n",
        "      video.append_data(py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = tf_env.step(action_step.action)\n",
        "        video.append_data(py_env.render())\n",
        "\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "yNw07pahBY78",
        "outputId": "a8613190-922f-4b0e-f280-4812cf86ab7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <video width=\"480\" height=\"480\" controls>\n",
              "      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "    Your browser does not support the video tag.\n",
              "    </video>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)\n",
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPHWdRyT-ND0"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MnqVY5icuPi4"
      },
      "outputs": [],
      "source": [
        "train_py_env = TicTacToeEnv1()\n",
        "eval_py_env = TicTacToeEnv1()\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZmdt4QB8h00",
        "outputId": "515ddc3d-a17f-429b-d566-aa7b38f11e38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'observation': {'legal_moves': TensorSpec(shape=(81,), dtype=tf.bool, name=None),\n",
              "                 'state': BoundedTensorSpec(shape=(9, 9, 2), dtype=tf.int64, name=None, minimum=array(0), maximum=array(1))},\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train_env.time_step_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "conv_layer_params = [16, 32]\n",
        "action_tensor_spec = tensor_spec.from_spec(tf_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Conv layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def conv_layer(num_units):\n",
        "  return tf.keras.layers.Conv2D(\n",
        "                      filters=num_units,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "\n",
        "\n",
        "# QNetwork consists of a sequence of Conv layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "normalization1 = tf.keras.layers.BatchNormalization()\n",
        "normalization2 = tf.keras.layers.BatchNormalization()\n",
        "conv_layers = [conv_layer(num_units) for num_units in conv_layer_params]\n",
        "action_conv = tf.keras.layers.Conv2D(filters=4,\n",
        "                    kernel_size=[1, 1], padding=\"same\",\n",
        "                    data_format=\"channels_last\",\n",
        "                    activation=tf.nn.leaky_relu)\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential([normalization1] + conv_layers + [action_conv, normalization2, flatten, q_values_layer])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNestedLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, *args, **kwds):\n",
        "        super().__init__(*args, **kwds)\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "                      filters=32,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "                      filters=64,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.norm1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BeZ-5rpD-58G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
        "    train_env.observation_spec()['state'],\n",
        "    train_env.action_spec(),\n",
        "    num_atoms=num_atoms,\n",
        "    preprocessing_layers = ConvNestedLayer(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "metadata": {
        "id": "NtrMeYVG4Jpr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lg5emNEam75J"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "# agent = dqn_agent.DqnAgent(\n",
        "#     train_env.time_step_spec(),\n",
        "#     train_env.action_spec(),\n",
        "#     q_network=q_net,\n",
        "#     optimizer=optimizer,\n",
        "#     td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "#     train_step_counter=train_step_counter)\n",
        "\n",
        "\n",
        "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    categorical_q_network=categorical_q_net,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
        "    optimizer=optimizer,\n",
        "    min_q_value=min_q_value,\n",
        "    max_q_value=max_q_value,\n",
        "    n_step_update=n_step_update,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVnWWX4xpa5"
      },
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uGU0pGS4xsjc"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "N8vRFWNgx-tj"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlCXIdRVyAAu",
        "outputId": "3a6e04d5-0c04-4908-960d-d21bf5872ec7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10000062"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "compute_avg_return(eval_env, agent.policy, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBEXcoMjy7xO"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "CMVmKJF1y_Lt"
      },
      "outputs": [],
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "def collect_step(environment, policy):\n",
        "    time_step = environment.current_time_step()\n",
        "    if time_step.is_last():\n",
        "        time_step = environment.reset()\n",
        "    action_step = policy.action(time_step)\n",
        "    next_time_step = environment.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "    # Add trajectory to the replay buffer\n",
        "    replay_buffer.add_batch(traj)\n",
        "\n",
        "for _ in range(initial_collect_steps):\n",
        "    collect_step(train_env, random_policy)\n",
        "\n",
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
        "    num_steps=n_step_update + 1).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the checkpointer and Policy saver"
      ],
      "metadata": {
        "id": "tmMc8b6mBqli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_zip_file(dirname, base_filename):\n",
        "  return shutil.make_archive(base_filename, 'zip', dirname)"
      ],
      "metadata": {
        "id": "dRLhR6v4BsEb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_dir = os.path.join(tempdir, 'policy')\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)"
      ],
      "metadata": {
        "id": "aI2LVHZXBv4q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRVMc8V8zxj0"
      },
      "source": [
        "## Train the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1qWkmKz0Ke",
        "outputId": "59fcd821-ad49-4539-bbaf-9520719807c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 200: loss = 1.6713066101074219\n",
            "step = 400: loss = 2.169821262359619\n",
            "step = 600: loss = 2.179478406906128\n",
            "step = 800: loss = 1.289841890335083\n",
            "step = 1000: loss = 1.757983684539795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation/legal_moves` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation_legal_moves`.\n",
            "WARNING:absl:`0/observation/state` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation_state`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1000: Average Return = -0.6199987530708313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 1200: loss = 2.1051955223083496\n",
            "step = 1400: loss = 2.4827213287353516\n",
            "step = 1600: loss = 2.1336159706115723\n",
            "step = 1800: loss = 2.0858232975006104\n",
            "step = 2000: loss = 1.8724911212921143\n",
            "step = 2000: Average Return = -1.139998197555542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 2200: loss = 1.9912718534469604\n",
            "step = 2400: loss = 1.8947548866271973\n",
            "step = 2600: loss = 1.918281078338623\n",
            "step = 2800: loss = 1.8067882061004639\n",
            "step = 3000: loss = 1.9801459312438965\n",
            "step = 3000: Average Return = -1.6499977111816406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 3200: loss = 1.8717169761657715\n",
            "step = 3400: loss = 1.8395583629608154\n",
            "step = 3600: loss = 1.8716801404953003\n",
            "step = 3800: loss = 1.7229559421539307\n",
            "step = 4000: loss = 1.8342792987823486\n",
            "step = 4000: Average Return = -0.4499988555908203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 4200: loss = 1.7555800676345825\n",
            "step = 4400: loss = 2.01725697517395\n",
            "step = 4600: loss = 3.460653066635132\n",
            "step = 4800: loss = 3.260847806930542\n",
            "step = 5000: loss = 3.071662425994873\n",
            "step = 5000: Average Return = -1.5299979448318481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 5200: loss = 2.9609644412994385\n",
            "step = 5400: loss = 2.8303542137145996\n",
            "step = 5600: loss = 2.743882417678833\n",
            "step = 5800: loss = 2.6765120029449463\n",
            "step = 6000: loss = 2.6164402961730957\n",
            "step = 6000: Average Return = -1.269998550415039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 6200: loss = 2.5690650939941406\n",
            "step = 6400: loss = 2.532017230987549\n",
            "step = 6600: loss = 2.5113003253936768\n",
            "step = 6800: loss = 2.497647285461426\n",
            "step = 7000: loss = 2.465839385986328\n",
            "step = 7000: Average Return = -0.009999299421906471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 7200: loss = 2.4562230110168457\n",
            "step = 7400: loss = 2.439967155456543\n",
            "step = 7600: loss = 2.410733222961426\n",
            "step = 7800: loss = 2.401519298553467\n",
            "step = 8000: loss = 2.3814334869384766\n",
            "step = 8000: Average Return = -0.14999902248382568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 8200: loss = 2.368307113647461\n",
            "step = 8400: loss = 2.3643717765808105\n",
            "step = 8600: loss = 2.3569412231445312\n",
            "step = 8800: loss = 2.3573858737945557\n",
            "step = 9000: loss = 2.3555212020874023\n",
            "step = 9000: Average Return = -0.03999912738800049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 9200: loss = 2.342989921569824\n",
            "step = 9400: loss = 2.332048177719116\n",
            "step = 9600: loss = 2.328770637512207\n",
            "step = 9800: loss = 2.324673652648926\n",
            "step = 10000: loss = 2.3243260383605957\n",
            "step = 10000: Average Return = -0.4299989640712738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 10200: loss = 2.317047119140625\n",
            "step = 10400: loss = 2.3083038330078125\n",
            "step = 10600: loss = 2.301445245742798\n",
            "step = 10800: loss = 2.296895980834961\n",
            "step = 11000: loss = 2.2892849445343018\n",
            "step = 11000: Average Return = -0.11999919265508652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 11200: loss = 2.27952241897583\n",
            "step = 11400: loss = 2.2747087478637695\n",
            "step = 11600: loss = 2.263507843017578\n",
            "step = 11800: loss = 2.2666831016540527\n",
            "step = 12000: loss = 2.276902914047241\n",
            "step = 12000: Average Return = -0.08999936282634735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 12200: loss = 2.2698616981506348\n",
            "step = 12400: loss = 2.27414870262146\n",
            "step = 12600: loss = 2.270479440689087\n",
            "step = 12800: loss = 2.255082130432129\n",
            "step = 13000: loss = 2.2497286796569824\n",
            "step = 13000: Average Return = -0.32999899983406067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 13200: loss = 2.2539353370666504\n",
            "step = 13400: loss = 2.2549548149108887\n",
            "step = 13600: loss = 2.2520692348480225\n",
            "step = 13800: loss = 2.251690626144409\n",
            "step = 14000: loss = 2.257721424102783\n",
            "step = 14000: Average Return = 0.18000061810016632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 14200: loss = 2.250591278076172\n",
            "step = 14400: loss = 2.245882034301758\n",
            "step = 14600: loss = 2.2445592880249023\n",
            "step = 14800: loss = 2.2478556632995605\n",
            "step = 15000: loss = 2.2502355575561523\n",
            "step = 15000: Average Return = -0.21999907493591309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 15200: loss = 2.2431411743164062\n",
            "step = 15400: loss = 2.243722915649414\n",
            "step = 15600: loss = 2.2402637004852295\n",
            "step = 15800: loss = 2.2366325855255127\n",
            "step = 16000: loss = 2.2389378547668457\n",
            "step = 16000: Average Return = -0.33999887108802795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 16200: loss = 2.2358200550079346\n",
            "step = 16400: loss = 2.2395167350769043\n",
            "step = 16600: loss = 2.244253635406494\n",
            "step = 16800: loss = 2.2423338890075684\n",
            "step = 17000: loss = 2.2384395599365234\n",
            "step = 17000: Average Return = 0.11000063270330429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 17200: loss = 2.2386553287506104\n",
            "step = 17400: loss = 2.238861322402954\n",
            "step = 17600: loss = 2.2427022457122803\n",
            "step = 17800: loss = 2.230999708175659\n",
            "step = 18000: loss = 2.2425143718719482\n",
            "step = 18000: Average Return = -0.41999882459640503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 18200: loss = 2.2410683631896973\n",
            "step = 18400: loss = 2.241469383239746\n",
            "step = 18600: loss = 2.230785846710205\n",
            "step = 18800: loss = 2.2246253490448\n",
            "step = 19000: loss = 2.232102394104004\n",
            "step = 19000: Average Return = 0.7100003361701965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 19200: loss = 2.231112241744995\n",
            "step = 19400: loss = 2.227442979812622\n",
            "step = 19600: loss = 2.21697998046875\n",
            "step = 19800: loss = 2.217949628829956\n",
            "step = 20000: loss = 2.2224948406219482\n",
            "step = 20000: Average Return = 0.3200004994869232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 20200: loss = 2.228635311126709\n",
            "step = 20400: loss = 2.23043155670166\n",
            "step = 20600: loss = 2.2261743545532227\n",
            "step = 20800: loss = 2.216956615447998\n",
            "step = 21000: loss = 2.223219871520996\n",
            "step = 21000: Average Return = 0.17000070214271545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 21200: loss = 2.2269272804260254\n",
            "step = 21400: loss = 2.226865291595459\n",
            "step = 21600: loss = 2.2292799949645996\n",
            "step = 21800: loss = 2.222665548324585\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    for _ in range(collect_steps_per_iteration):\n",
        "      collect_step(train_env, agent.collect_policy)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "      print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "      print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "      tf_policy_saver.save(policy_dir)\n",
        "      policy_zip_filename = create_zip_file(policy_dir, os.path.join(tempdir, 'exported_policy'))\n",
        "      returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the trained model"
      ],
      "metadata": {
        "id": "euOj19B0B1ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDMa0xH3rAS6"
      },
      "outputs": [],
      "source": [
        "saved_policy = tf.saved_model.load(policy_dir)\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsl0r1fyxpUD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkOaR2zHrDwC"
      },
      "source": [
        "## TODO\n",
        "\n",
        "\n",
        "\n",
        "*   Check whether all functions are correct\n",
        "*   Check whether the environment is correctly implement\n",
        "*   Add the variant in `step` function\n",
        "*   Test whether the agent always chooses an *empty* square and whether they can go to an adjacent place correctly\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWeeqNTNss0au0aSzYg9yy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}