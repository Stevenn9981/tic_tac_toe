{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stevenn9981/tic_tac_toe/blob/master/tic_tac_toe_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEy_PMnnf-Og"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdG6QronfqnR",
        "outputId": "bc37f8cf-fb84-413a-feaf-f113b689f4ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ub\r                                                                               \rHit:2 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Waiting for headers] [Con\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-6).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.22.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.13.0)\n",
            "Requirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-agents[reverb]) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.13.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install imageio\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-vtSiPUgOOU"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import zipfile\n",
        "\n",
        "import pygame as pg\n",
        "from pygame import gfxdraw\n",
        "from pygame.locals import *\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import categorical_q_network\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "tempdir = \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsV4e7iqg10s"
      },
      "outputs": [],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWwU8Wp7g2_X"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reKJx_ybg7_L"
      },
      "outputs": [],
      "source": [
        "num_iterations = 3000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "initial_collect_episodes = 5  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "collect_episodes_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 5e-4  # @param {type:\"number\"}\n",
        "log_interval = 10  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 50  # @param {type:\"integer\"}\n",
        "eval_interval = 50  # @param {type:\"integer\"}\n",
        "\n",
        "gamma = 0.99    # @param {type:\"number\"}\n",
        "n_step_update = 7  # @param {type:\"integer\"}\n",
        "num_atoms = 51  # @param {type:\"integer\"}\n",
        "min_q_value = -5  # @param {type:\"integer\"}\n",
        "max_q_value = 5  # @param {type:\"integer\"}\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "BOARD_SIZE = 9 # @param {type:\"integer\"}\n",
        "\n",
        "REWARD_DRAW = 0    # @param {type:\"number\"}\n",
        "REWARD_ALIVE = -0.04    # @param {type:\"number\"}\n",
        "REWARD_NON_ADJ = -0.02    # @param {type:\"number\"}\n",
        "\n",
        "# '_' means empty position, 'O' and 'X' means two players.\n",
        "REWARD_ACTIVE_TWO = 0.1 # @param {type:\"number\"} _OO_ or _XX_, we call it active_two\n",
        "REWARD_NONACT_THREE = 0.2 # @param {type:\"number\"} _OOOX or _XXXO or XOOO_ or OXXX_, we call it non_active_three\n",
        "REWARD_ACTIVE_THREE = 0.5 # @param {type:\"number\"} _OOO_ or _XXX_, we call it active_three\n",
        "REWARD_WIN = 1    # @param {type:\"number\"} _OOOO_ or _XXXX_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOBVDKmMIcyJ"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq7Q2oxyIhS8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "class TicTacToeEnv1(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    Implementation of a TicTacToe Environment based on OpenAI Gym standards\n",
        "    This class is modified from https://github.com/MauroLuzzatto/OpenAI-Gym-TicTacToe-Environment\n",
        "    I modified that to follow the instructions of Question 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"This class contains a TicTacToe environment for OpenAI Gym\n",
        "\n",
        "        Args:\n",
        "            mode (str): render mode: human or rgb_array, which returns a text or RGB array of a picture that shows the current board\n",
        "        \"\"\"\n",
        "        self.n_actions = BOARD_SIZE * BOARD_SIZE         # 9 * 9 grids to drop\n",
        "\n",
        "        self._observation_spec = {'state': array_spec.BoundedArraySpec(shape=(BOARD_SIZE, BOARD_SIZE, 4), dtype=np.int_, minimum=0, maximum=1),\n",
        "                                  'legal_moves': array_spec.ArraySpec(shape=(self.n_actions,), dtype=np.bool_)}\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int_, minimum=0, maximum=80)\n",
        "        self.colors = [1, 2]\n",
        "        self.screen = None\n",
        "        self.fields_per_side = BOARD_SIZE\n",
        "        self.reset()\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def get_result(self):\n",
        "        return self.result\n",
        "\n",
        "    def if_chess_nearby(self, row, col) -> bool:\n",
        "        \"\"\"\n",
        "        Determine whether there are chess pieces in 2 squares around the given position (row, col)\n",
        "\n",
        "        Args:\n",
        "          row (int): row index\n",
        "          col (int): column index\n",
        "\n",
        "        Returns:\n",
        "          res (boolean): true, if there are\n",
        "        \"\"\"\n",
        "        for i in range(-2, 3):\n",
        "            for j in range(-2, 3):\n",
        "                if 0 <= row + i < BOARD_SIZE and 0 <= col + j < BOARD_SIZE:\n",
        "                    if self.board[row + i, col + j] != 0:\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "    def _reset(self) -> Tuple[np.ndarray, dict]:\n",
        "        \"\"\"\n",
        "        reset the board game and state\n",
        "        \"\"\"\n",
        "        self.board: np.ndarray = np.zeros(\n",
        "            (self.fields_per_side, self.fields_per_side), dtype=int\n",
        "        )\n",
        "        self.current_player = 1\n",
        "        self.info = {\"players\": {1: {\"actions\": []}, 2: {\"actions\": []}}, \"Occupied\": set(), \"legal_moves\": np.ones((self.n_actions,), dtype=bool)}\n",
        "        self.latest_action = None\n",
        "\n",
        "        # 0 means not finished, 1 or 2 means the winner and 3 means draw\n",
        "        self.result = 0\n",
        "\n",
        "        # return self.decompose_board_to_state()\n",
        "        observations_and_legal_moves = {'state': self.decompose_board_to_state(), 'legal_moves': self.info[\"legal_moves\"]}\n",
        "        return ts.restart(observations_and_legal_moves)\n",
        "\n",
        "    def decompose_board_to_state(self):\n",
        "        \"\"\"\n",
        "        Our state is a 9x9x4 matrix.\n",
        "        The first layer is the opponent's play history, 0 means no stone, 1 means stones placed by the opponent.\n",
        "        The second layer is the current player's history, 0 means no stone, 1 means stones placed by the current player.\n",
        "        The third layer is the opponent's latest play-out; only one entry is 1 and the others are 0.\n",
        "        The fourth layer is whether the current player is the first hand. an array that is full of 1 means yes, and 0 means no.\n",
        "        \"\"\"\n",
        "        opponent = 2 if self.current_player == 1 else 1\n",
        "        o_plays = (self.board == opponent) * 1\n",
        "        c_plays = (self.board == self.current_player) * 1\n",
        "        l_play = np.zeros_like(self.board)\n",
        "        if self.latest_action:\n",
        "            r, c = self.decode_action(self.latest_action)\n",
        "            l_play[r, c] = 1\n",
        "        if_first = np.full_like(self.board, (self.current_player == 1) * 1)\n",
        "        return np.stack([o_plays, c_plays, l_play, if_first], axis=2)\n",
        "\n",
        "\n",
        "    def _step(self, action: int) -> Tuple[np.ndarray, int, bool, dict]:\n",
        "        \"\"\"step function of the tictactoeEnv1\n",
        "\n",
        "        Args:\n",
        "          action (int): integer between [0, 80], each representing a field on the board\n",
        "\n",
        "        Returns:\n",
        "          state (np.array): state of 2 players' history, 0 means no stone, 1 means stones placed by the corresponding player (shape: 9x9x2).\n",
        "          reward (int): reward of the currrent step\n",
        "          done (boolean): true, if the game is finished\n",
        "          (dict): empty dict for future game related information\n",
        "        \"\"\"\n",
        "        action = int(action)\n",
        "        if not (0 <= action < self.n_actions):\n",
        "            raise ValueError(f\"action '{action}' is not in action_space\")\n",
        "\n",
        "        reward = REWARD_ALIVE\n",
        "        (row, col) = self.decode_action(action)\n",
        "\n",
        "        # If the agent/player does not choose an empty square, randomly select an empty one.\n",
        "        if self.board[row, col] != 0:\n",
        "            if len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE:\n",
        "                raise ValueError('BORAD IS FULL!')\n",
        "            raise ValueError('ERROR: Not A LEGAL MOVE (NOT EMPTY)')\n",
        "            # action = random.choice(list(set(range(BOARD_SIZE * BOARD_SIZE)) - self.info[\"Occupied\"]))\n",
        "            # (row, col) = self.decode_action(action)\n",
        "            # reward -= 5 # assign a negative reward if the play-out position is not empty\n",
        "\n",
        "        # According to the game rules, randomly select an adjacent position with a probability of 1/16\n",
        "        if random.random() < 0.5:\n",
        "            row, col = self.choose_adj_pos(row, col)\n",
        "\n",
        "        # if len(self.info[\"Occupied\"]) != 0 and not self.if_chess_nearby(row, col):\n",
        "        #     reward += REWARD_NON_ADJ\n",
        "\n",
        "        win = False\n",
        "        if 0 <= row < BOARD_SIZE and 0 <= col < BOARD_SIZE and self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.current_player  # drop the piece on the field\n",
        "            win = self._is_win(self.current_player, row, col)\n",
        "\n",
        "            cnt_act_two = self.detect_alive_two(row, col)\n",
        "            cnt_non_act_three, cnt_act_three = self.detect_three(row, col)\n",
        "\n",
        "            reward += (cnt_act_two * REWARD_ACTIVE_TWO + cnt_non_act_three * REWARD_NONACT_THREE + cnt_act_three * REWARD_ACTIVE_THREE)\n",
        "\n",
        "            action = row * BOARD_SIZE + col\n",
        "            self.latest_action = action\n",
        "            self.info[\"players\"][self.current_player][\"actions\"].append(action)\n",
        "            self.info[\"Occupied\"].add(action)\n",
        "            self.info['legal_moves'][action] = False\n",
        "\n",
        "        if win:\n",
        "            self.result = self.current_player\n",
        "            reward += REWARD_WIN\n",
        "        elif len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE: # Draw\n",
        "            self.result = 3\n",
        "            reward += REWARD_DRAW\n",
        "\n",
        "        done = (win or len(self.info[\"Occupied\"]) == BOARD_SIZE * BOARD_SIZE)\n",
        "        self.current_player = self.current_player + 1 if self.current_player == 1 else 1\n",
        "        state = self.decompose_board_to_state()\n",
        "\n",
        "        observations_and_legal_moves = {'state': state, 'legal_moves': self.info['legal_moves']}\n",
        "\n",
        "        if done:\n",
        "            return ts.termination(observations_and_legal_moves, reward)\n",
        "        else:\n",
        "            return ts.transition(observations_and_legal_moves, reward)\n",
        "\n",
        "    def detect_alive_two(self, row: int, col: int) -> int:\n",
        "        \"\"\" Detect how many alive_two can obtain by this play out.\n",
        "\n",
        "        Args:\n",
        "            row (int): row of the current play\n",
        "            col (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            cnt (int): the number of alive_two\n",
        "        \"\"\"\n",
        "        cnt = 0\n",
        "        adjs = [[-1, -1], [-1, 0], [-1, 1], [0, 1], [0, -1], [1, -1], [1, 0], [1, 1]]\n",
        "        for adj in adjs:\n",
        "            p1_r, p1_c = row + 2 * adj[0], col + 2 * adj[1]\n",
        "            p2_r, p2_c = row - adj[0], col - adj[1]\n",
        "            if 0 <= p1_r < 9 and 0 <= p1_c < 9 and 0 <= p2_r < 9 and 0 <= p2_c < 9:\n",
        "                if self.board[row + adj[0], col + adj[1]] == self.current_player and self.board[p1_r, p1_c] == 0 and self.board[p2_r, p2_c] == 0:\n",
        "                    cnt += 1\n",
        "        return cnt\n",
        "\n",
        "    def detect_three(self, r: int, c: int) -> tuple:\n",
        "        \"\"\" Detect how many non_active_three and active_three can obtain by this play out.\n",
        "\n",
        "        Args:\n",
        "            r (int): row of the current play\n",
        "            c (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            cnt_non_act (int): the number of non_active_three\n",
        "            cnt_act (int): the number of active_three\n",
        "        \"\"\"\n",
        "        cnt_non_act = 0\n",
        "        cnt_act = 0\n",
        "        opponent = self.current_player + 1 if self.current_player == 1 else 1\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-2, 3):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if self.board[r + offset * direct[0], c + offset * direct[1]] == self.current_player:\n",
        "                        count += 1\n",
        "                        if count == 3:\n",
        "                            p1_r, p1_c = r + (offset + 1) * direct[0], c + (offset + 1) * direct[1]\n",
        "                            p2_r, p2_c = r + (offset - 3) * direct[0], c + (offset - 3) * direct[1]\n",
        "\n",
        "                            p1_is_empty = (0 <= p1_r < 9 and 0 <= p1_c < 9 and self.board[p1_r, p1_c] == 0)\n",
        "                            p2_is_empty = (0 <= p2_r < 9 and 0 <= p2_c < 9 and self.board[p2_r, p2_c] == 0)\n",
        "\n",
        "                            if p1_is_empty and p2_is_empty:\n",
        "                                cnt_act += 1\n",
        "                            elif p1_is_empty or p2_is_empty:\n",
        "                                cnt_non_act += 1\n",
        "                            break\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return cnt_non_act, cnt_act\n",
        "\n",
        "    def choose_adj_pos(self, row: int, col: int) -> tuple:\n",
        "        \"\"\" Randomly select an adjacent position with equal probabilities.\n",
        "\n",
        "        Args:\n",
        "            row (int): row of the current play\n",
        "            col (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            row (int): row of the selected adjacent position\n",
        "            col (int): column of the selected adjacent position\n",
        "        \"\"\"\n",
        "\n",
        "        adjs = [[-1, -1], [-1, 0], [-1, 1], [0, 1], [0, -1], [1, -1], [1, 0], [1, 1]]\n",
        "        adj = random.choice(adjs)\n",
        "        row, col = row + adj[0], col + adj[1]\n",
        "        return row, col\n",
        "\n",
        "    def _is_win(self, color: int, r: int, c: int) -> bool:\n",
        "        \"\"\"check if this player results in a winner\n",
        "\n",
        "        Args:\n",
        "            color (int): of the player\n",
        "            r (int): row of the current play\n",
        "            c (int): column of the current play\n",
        "\n",
        "        Returns:\n",
        "            bool: indicating if there is a winner\n",
        "        \"\"\"\n",
        "\n",
        "        # check if four equal stones are aligned (horizontal, verical or diagonal)\n",
        "        directions = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "\n",
        "        for direct in directions:\n",
        "            count = 0\n",
        "            for offset in range(-3, 4):\n",
        "                if 0 <= r + offset * direct[0] < 9 and 0 <= c + offset * direct[1] < 9:\n",
        "                    if self.board[r + offset * direct[0], c + offset * direct[1]] == color:\n",
        "                        count += 1\n",
        "                        if count == 4:\n",
        "                            return True\n",
        "                    else:\n",
        "                        count = 0\n",
        "\n",
        "        return False\n",
        "\n",
        "    def decode_action(self, action: int) -> List[int]:\n",
        "        \"\"\"decode the action integer into a colum and row value\n",
        "\n",
        "        0 = upper left corner\n",
        "        8 = lower right corner\n",
        "\n",
        "        Args:\n",
        "            action (int): action\n",
        "\n",
        "        Returns:\n",
        "            List[int, int]: a list with the [row, col] values\n",
        "        \"\"\"\n",
        "        col = action % BOARD_SIZE\n",
        "        row = action // BOARD_SIZE\n",
        "        assert 0 <= col < BOARD_SIZE\n",
        "        return [row, col]\n",
        "\n",
        "    def render(self, render_mode=\"rgb_array\") -> None:\n",
        "        \"\"\"Render the board\n",
        "        Print a string that shows the current board, if render_mode == human,\n",
        "        Return the RGB array of a figure which shows the current board.\n",
        "        \"\"\"\n",
        "        board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=str)\n",
        "        for ii in range(BOARD_SIZE):\n",
        "            for jj in range(BOARD_SIZE):\n",
        "                if self.board[ii, jj] == 0:\n",
        "                    board[ii, jj] = \"-\"\n",
        "                elif self.board[ii, jj] == 1:\n",
        "                    board[ii, jj] = \"X\"\n",
        "                elif self.board[ii, jj] == 2:\n",
        "                    board[ii, jj] = \"O\"\n",
        "\n",
        "        if render_mode == \"human\":\n",
        "            board = tabulate(board, tablefmt=\"fancy_grid\")\n",
        "            print(board)\n",
        "            print(\"\\n\")\n",
        "\n",
        "\n",
        "        width = height = 400\n",
        "\n",
        "        white = (255, 255, 255)\n",
        "        line_color = (0, 0, 0)\n",
        "\n",
        "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "        pg.init()\n",
        "\n",
        "        # Set up the drawing window\n",
        "        if self.screen is None:\n",
        "          self.screen = pg.display.set_mode([width + 16, height + 16])\n",
        "\n",
        "        self.screen.fill(white)\n",
        "        # drawing vertical lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (width / BOARD_SIZE * i, 0), (width / BOARD_SIZE * i, height), 2)\n",
        "\n",
        "        # drawing horizontal lines\n",
        "        for i in range(10):\n",
        "          pg.draw.line(self.screen, line_color, (0, height / BOARD_SIZE * i), (width, height / BOARD_SIZE * i), 2)\n",
        "        pg.display.flip()\n",
        "\n",
        "        # drawing noughts and crosses\n",
        "        for i in range(BOARD_SIZE):\n",
        "          for j in range(BOARD_SIZE):\n",
        "            if self.board[i, j] == 1: # Draw crosses\n",
        "              pg.draw.lines(self.screen, line_color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) - 10),\n",
        "                                                      (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10)], 3)\n",
        "              pg.draw.lines(self.screen, line_color, True, [(width / BOARD_SIZE * (j + 0.5) - 10,\n",
        "                                                        height / BOARD_SIZE * (i + 0.5) + 10),\n",
        "                                                        (width / BOARD_SIZE * (j + 0.5) + 10,\n",
        "                                                          height / BOARD_SIZE * (i + 0.5) - 10)], 3)\n",
        "            elif self.board[i, j] == 2: # Draw noughts\n",
        "              pg.draw.circle(self.screen, line_color, (width / BOARD_SIZE * (j + 0.5), height / BOARD_SIZE * (i + 0.5)), 12, 3)\n",
        "\n",
        "        board = np.transpose(\n",
        "                np.array(pg.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "        return board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhocpw-j36C0"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "    return obs['state'], obs['legal_moves']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFsRLWJslkf"
      },
      "source": [
        "## Random play test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU1h9wSWa7Lh"
      },
      "outputs": [],
      "source": [
        "# py_env = suite_gym.wrap_env(TicTacToeEnv1())\n",
        "py_env = TicTacToeEnv1()\n",
        "tf_env = tf_py_environment.TFPyEnvironment(py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYsgWM-79vr"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"480\" height=\"480\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, fps=2):\n",
        "    py_env = TicTacToeEnv1()\n",
        "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "      time_step = tf_env.reset()\n",
        "      video.append_data(py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = tf_env.step(action_step.action)\n",
        "        video.append_data(py_env.render())\n",
        "\n",
        "    return embed_mp4(filename)\n",
        "\n",
        "\n",
        "def create_policy_battle_video(policy1, policy2, filename, fps=2):\n",
        "    py_env = TicTacToeEnv1()\n",
        "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "      time_step = tf_env.reset()\n",
        "      video.append_data(py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy1.action(time_step)\n",
        "        time_step = tf_env.step(action_step.action)\n",
        "        video.append_data(py_env.render())\n",
        "        if not time_step.is_last():\n",
        "          action_step = policy2.action(time_step)\n",
        "          time_step = tf_env.step(action_step.action)\n",
        "          video.append_data(py_env.render())\n",
        "\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNw07pahBY78"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)\n",
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPHWdRyT-ND0"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnqVY5icuPi4"
      },
      "outputs": [],
      "source": [
        "train_py_env = TicTacToeEnv1()\n",
        "eval_py_env = TicTacToeEnv1()\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "conv_layer_params = [16, 32]\n",
        "action_tensor_spec = tensor_spec.from_spec(tf_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Conv layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def conv_layer(num_units):\n",
        "  return tf.keras.layers.Conv2D(\n",
        "                      filters=num_units,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "\n",
        "\n",
        "# QNetwork consists of a sequence of Conv layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "normalization1 = tf.keras.layers.BatchNormalization()\n",
        "normalization2 = tf.keras.layers.BatchNormalization()\n",
        "conv_layers = [conv_layer(num_units) for num_units in conv_layer_params]\n",
        "action_conv = tf.keras.layers.Conv2D(filters=4,\n",
        "                    kernel_size=[1, 1], padding=\"same\",\n",
        "                    data_format=\"channels_last\",\n",
        "                    activation=tf.nn.leaky_relu)\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential([normalization1] + conv_layers + [action_conv, normalization2, flatten, q_values_layer])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeZ-5rpD-58G"
      },
      "outputs": [],
      "source": [
        "class ConvNestedLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, *args, **kwds):\n",
        "        super().__init__(*args, **kwds)\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "                      filters=32,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "                      filters=64,\n",
        "                      kernel_size=[3, 3],\n",
        "                      padding=\"same\",\n",
        "                      data_format=\"channels_last\",\n",
        "                      activation=tf.nn.leaky_relu,\n",
        "                      dtype=float)\n",
        "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.norm1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtrMeYVG4Jpr"
      },
      "outputs": [],
      "source": [
        "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
        "    train_env.observation_spec()['state'],\n",
        "    train_env.action_spec(),\n",
        "    num_atoms=num_atoms,\n",
        "    preprocessing_layers = ConvNestedLayer(),\n",
        "    fc_layer_params=fc_layer_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg5emNEam75J"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# agent = dqn_agent.DqnAgent(\n",
        "#     train_env.time_step_spec(),\n",
        "#     train_env.action_spec(),\n",
        "#     q_network=q_net,\n",
        "#     observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
        "#     optimizer=optimizer,\n",
        "#     td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "#     train_step_counter=train_step_counter)\n",
        "\n",
        "\n",
        "agent1 = categorical_dqn_agent.CategoricalDqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    categorical_q_network=categorical_q_net,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
        "    optimizer=optimizer,\n",
        "    min_q_value=min_q_value,\n",
        "    max_q_value=max_q_value,\n",
        "    n_step_update=n_step_update,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=tf.Variable(0))\n",
        "\n",
        "agent2 = categorical_dqn_agent.CategoricalDqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    categorical_q_network=categorical_q_net,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
        "    optimizer=optimizer,\n",
        "    min_q_value=min_q_value,\n",
        "    max_q_value=max_q_value,\n",
        "    n_step_update=n_step_update,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=tf.Variable(0))\n",
        "\n",
        "agent1.initialize()\n",
        "agent2.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVnWWX4xpa5"
      },
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGU0pGS4xsjc"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8vRFWNgx-tj"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "def compute_avg_return_battle(environment, policy1, policy2, num_episodes=10):\n",
        "\n",
        "  total_return_1 = 0.0\n",
        "  total_return_2 = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    episode_return_1 = 0.0\n",
        "    episode_return_2 = 0.0\n",
        "    time_step = environment.reset()\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy1.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return_1 += time_step.reward\n",
        "      if not time_step.is_last():\n",
        "        action_step = policy2.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "        episode_return_2 += time_step.reward\n",
        "    total_return_1 += episode_return_1\n",
        "    total_return_2 += episode_return_2\n",
        "\n",
        "  avg_return_1 = total_return_1 / num_episodes\n",
        "  avg_return_2 = total_return_2 / num_episodes\n",
        "  return [avg_return_1.numpy()[0], avg_return_2.numpy()[0]]\n",
        "\n",
        "def compute_avg_win_battle(environment, policy1, policy2, num_episodes=10):\n",
        "\n",
        "  total_return_1 = 0.0\n",
        "  total_return_2 = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy1.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      if not time_step.is_last():\n",
        "        action_step = policy2.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "    if environment._envs[0].get_result() == 1:\n",
        "        total_return_1 += 1\n",
        "    elif environment._envs[0].get_result() == 2:\n",
        "        total_return_2 += 1\n",
        "\n",
        "  avg_return_1 = total_return_1 / num_episodes\n",
        "  avg_return_2 = total_return_2 / num_episodes\n",
        "  return [avg_return_1, avg_return_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCXIdRVyAAu"
      },
      "outputs": [],
      "source": [
        "print(compute_avg_return(eval_env, agent1.policy, 1))\n",
        "print(compute_avg_return(eval_env, agent2.policy, 1))\n",
        "print(compute_avg_return_battle(eval_env, agent1.policy, agent2.policy, 10))\n",
        "print(compute_avg_win_battle(eval_env, agent1.policy, agent2.policy, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBEXcoMjy7xO"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMVmKJF1y_Lt"
      },
      "outputs": [],
      "source": [
        "replay_buffer1 = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent1.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "replay_buffer2 = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent2.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "def collect_episode(environment, policy1, replay_buffer1, policy2, replay_buffer2):\n",
        "    time_step = environment.reset()\n",
        "    trajs_1, trajs_2 = [], []\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = agent1.collect_policy.action(time_step)\n",
        "      next_time_step = environment.step(action_step.action)\n",
        "      traj1 = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "      trajs_1.append(traj1)\n",
        "      time_step = next_time_step\n",
        "\n",
        "      if not time_step.is_last():\n",
        "        action_step = agent2.collect_policy.action(time_step)\n",
        "        next_time_step = environment.step(action_step.action)\n",
        "        traj2 = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "        trajs_2.append(traj2)\n",
        "        time_step = next_time_step\n",
        "\n",
        "    # print('Before update:')\n",
        "    # for tsj in trajs_1:\n",
        "    #     print(f'Policy 1: {tsj.reward}')\n",
        "    # for tsj in trajs_2:\n",
        "    #     print(f'Policy 2: {tsj.reward}')\n",
        "    # print('After update:')\n",
        "\n",
        "    if len(trajs_1) == len(trajs_2): # Player 2 won\n",
        "        for i in range(len(trajs_1) - 1):\n",
        "            trajs_1[i] = trajs_1[i].replace(reward=trajs_1[i].reward - trajs_2[i].reward)\n",
        "            trajs_2[i] = trajs_2[i].replace(reward=trajs_2[i].reward - trajs_1[i + 1].reward)\n",
        "        trajs_1[-1] = trajs_1[-1].replace(reward=trajs_1[-1].reward - trajs_2[-1].reward)\n",
        "    else: # Player 1 won\n",
        "        for i in range(len(trajs_1) - 1):\n",
        "            trajs_1[i] = trajs_1[i].replace(reward=trajs_1[i].reward - trajs_2[i].reward)\n",
        "            trajs_2[i] = trajs_2[i].replace(reward=trajs_2[i].reward - trajs_1[i + 1].reward)\n",
        "\n",
        "\n",
        "    for tsj in trajs_1:\n",
        "        # print(f'Policy 1: {tsj.reward}')\n",
        "        replay_buffer1.add_batch(tsj)\n",
        "    for tsj in trajs_2:\n",
        "        # print(f'Policy 2: {tsj.reward}')\n",
        "        replay_buffer2.add_batch(tsj)\n",
        "\n",
        "    # print()\n",
        "\n",
        "for _ in range(initial_collect_episodes):\n",
        "    collect_episode(train_env, agent1.collect_policy, replay_buffer1, agent2.collect_policy, replay_buffer2)\n",
        "\n",
        "dataset1 = replay_buffer1.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
        "    num_steps=n_step_update + 1).prefetch(3)\n",
        "iterator1 = iter(dataset1)\n",
        "\n",
        "dataset2 = replay_buffer2.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
        "    num_steps=n_step_update + 1).prefetch(3)\n",
        "iterator2 = iter(dataset2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmMc8b6mBqli"
      },
      "source": [
        "## Set up the checkpointer and Policy saver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRLhR6v4BsEb"
      },
      "outputs": [],
      "source": [
        "def create_zip_file(dirname, base_filename):\n",
        "  return shutil.make_archive(base_filename, 'zip', dirname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI2LVHZXBv4q"
      },
      "outputs": [],
      "source": [
        "policy_dir_1 = os.path.join(tempdir, 'policy_1')\n",
        "tf_policy_saver_1 = policy_saver.PolicySaver(agent1.policy)\n",
        "\n",
        "policy_dir_2 = os.path.join(tempdir, 'policy_2')\n",
        "tf_policy_saver_2 = policy_saver.PolicySaver(agent2.policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRVMc8V8zxj0"
      },
      "source": [
        "## Train the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1qWkmKz0Ke"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent1.train = common.function(agent1.train)\n",
        "agent2.train = common.function(agent2.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent1.train_step_counter.assign(0)\n",
        "agent2.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy before training.\n",
        "avg_return = compute_avg_win_battle(eval_env, agent1.policy, agent2.policy, 5)\n",
        "policy1_win_rate = compute_avg_win_battle(eval_env, agent1.policy, random_policy, num_eval_episodes)[0]\n",
        "policy2_win_rate = compute_avg_win_battle(eval_env, random_policy, agent2.policy, num_eval_episodes)[1]\n",
        "print('Before training: 1_win = {0}, 2_win = {1}, Average Return = {2}'.format(policy1_win_rate, policy2_win_rate, str(avg_return)))\n",
        "returns = [avg_return]\n",
        "\n",
        "bst1, bst2 = policy1_win_rate, policy2_win_rate\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_env.reset()\n",
        "\n",
        "for idx in range(num_iterations):\n",
        "\n",
        "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
        "    for _ in range(collect_episodes_per_iteration):\n",
        "      if idx % 2:\n",
        "          collect_episode(train_env, agent1.collect_policy, replay_buffer1, agent2.collect_policy, replay_buffer2)\n",
        "      else:\n",
        "          collect_episode(train_env, agent2.collect_policy, replay_buffer2, agent1.collect_policy, replay_buffer1)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator1)\n",
        "    train_loss_1 = agent1.train(experience).loss\n",
        "    experience, unused_info = next(iterator2)\n",
        "    train_loss_2 = agent2.train(experience).loss\n",
        "\n",
        "    step = agent1.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "      print('step = {0}: loss = {1}'.format(step, (train_loss_1 + train_loss_2) / 2))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      avg_return = compute_avg_return_battle(eval_env, agent1.policy, agent2.policy, 5)\n",
        "      policy1_win_rate = compute_avg_win_battle(eval_env, agent1.policy, random_policy, num_eval_episodes)[0]\n",
        "      policy2_win_rate = compute_avg_win_battle(eval_env, random_policy, agent2.policy, num_eval_episodes)[1]\n",
        "      print('step = {0}: 1_win = {1}, 2_win = {2}, Average Return = {3}'.format(step, policy1_win_rate, policy2_win_rate, str(avg_return)))\n",
        "      if policy1_win_rate > bst1:\n",
        "        bst1 = policy1_win_rate\n",
        "        tf_policy_saver_1.save(policy_dir_1)\n",
        "        policy_zip_filename = create_zip_file(policy_dir_1, os.path.join(tempdir, 'exported_policy_1'))\n",
        "      if policy2_win_rate > bst2:\n",
        "        bst2 = policy2_win_rate\n",
        "        tf_policy_saver_2.save(policy_dir_2)\n",
        "        policy_zip_filename = create_zip_file(policy_dir_2, os.path.join(tempdir, 'exported_policy_2'))\n",
        "      returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOj19B0B1ky"
      },
      "source": [
        "## Load the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSRWvH6Udnd1"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec(),\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDMa0xH3rAS6"
      },
      "outputs": [],
      "source": [
        "saved_policy_1 = tf.saved_model.load(policy_dir_1)\n",
        "print(compute_avg_win_battle(eval_env, saved_policy_1, random_policy, 200))\n",
        "create_policy_eval_video(saved_policy_1, \"trained-agent-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUd9jzRPMdTa"
      },
      "outputs": [],
      "source": [
        "saved_policy_2 = tf.saved_model.load(policy_dir_2)\n",
        "print(compute_avg_win_battle(eval_env, saved_policy_2, random_policy, 200))\n",
        "create_policy_eval_video(saved_policy_2, \"trained-agent-2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_policy_battle_video(saved_policy_1, random_policy, 'trained-agent-1-with-random')"
      ],
      "metadata": {
        "id": "ZMvc6ac05OAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_policy_battle_video(saved_policy_2, random_policy, 'trained-agent-2-with-random')"
      ],
      "metadata": {
        "id": "J7eVTU-d5OiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_policy_battle_video(saved_policy_1, saved_policy_2, 'trained-agent-battle')"
      ],
      "metadata": {
        "id": "6SAhQ2Ph5Ft2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsl0r1fyxpUD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkOaR2zHrDwC"
      },
      "source": [
        "## TODO\n",
        "\n",
        "\n",
        "\n",
        "*   Check whether all functions are correct\n",
        "*   Check whether the environment is correctly implement\n",
        "*   Add the variant in `step` function\n",
        "*   Test whether the agent always chooses an *empty* square and whether they can go to an adjacent place correctly\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNspJNVetpiZTci82jyjKCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}